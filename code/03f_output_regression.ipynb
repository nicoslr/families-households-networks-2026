{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03f - Output: Regression analysis with linked survey data\n",
    "\n",
    "- **Project:** _Families, households, networks: Rethinking the relational structure of families through large-scale network data_ <br>\n",
    "- **Authors:** Nicol√°s Soler (ORCID 0009-0001-4239-9396), Tom Emery, Agnieszka Kanas <br>\n",
    "- **Last updated:** January 2026 <br>\n",
    "- **Full research article published in journal:** _Demography_ (2026)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.iolib.summary2 import summary_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YAML configuration\n",
    "path_config = \"config.yml\"\n",
    "with open(path_config, \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sample\n",
    "dtypes_sample = {\n",
    "    \"RINPERSOON\":pl.String,\n",
    "    \"nomem_encr\":pl.String,\n",
    "    \"id_hhd\":pl.Int64,\n",
    "    \"is_ego_survey\":pl.Int64,\n",
    "    \"hhd_size\":pl.Int64,\n",
    "    \"net_size_hhd_1\":pl.Int64,\n",
    "    \"net_size_hhd_2\":pl.Int64,\n",
    "    \"net_size_hhd_3\":pl.Int64,\n",
    "    \"net_size_hhd_4\":pl.Int64,\n",
    "    \"density_2\":pl.Float64,\n",
    "    \"density_3\":pl.Float64,\n",
    "    \"density_4\":pl.Float64,\n",
    "    \"overlap_survey_1\":pl.Int64,\n",
    "    \"overlap_survey_2\":pl.Int64,\n",
    "    \"overlap_survey_3\":pl.Int64,\n",
    "    \"overlap_survey_4\":pl.Int64,\n",
    "}\n",
    "\n",
    "sample = pl.scan_csv(config[\"data\"][\"sample\"], separator=\",\", encoding=\"utf8\", schema_overrides=dtypes_sample).select(dtypes_sample.keys()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read outcomes\n",
    "\n",
    "dtypes_outcomes = {\n",
    "    \"nomem_encr\":pl.String,\n",
    "    \"y_perc_fam_comp\":pl.Float64,\n",
    "    \"y_perc_contact_comp\":pl.Float64,\n",
    "    \"y_perc_supp_comp\":pl.Float64\n",
    "}\n",
    "\n",
    "outcomes = (\n",
    "    pl\n",
    "    .scan_csv(config[\"data\"][\"survey_outcomes\"], separator=\",\", encoding=\"utf8\", schema_overrides=dtypes_outcomes)\n",
    "    .select(dtypes_outcomes.keys())\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge outcomes to sample\n",
    "data = sample.join(outcomes, how=\"inner\", on=\"nomem_encr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing overlap as zero overlap\n",
    "data = data.with_columns(pl.col([\"overlap_survey_1\",\"overlap_survey_2\",\"overlap_survey_3\",\"overlap_survey_4\"]).fill_null(strategy=\"zero\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only complete cases and relevant vars\n",
    "data = data.drop_nulls().drop([\"nomem_encr\",\"id_hhd\",\"is_ego_survey\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative network size\n",
    "data = data.with_columns(\n",
    "    net_size_hhd_cum_2 = pl.sum_horizontal([\"net_size_hhd_1\", \"net_size_hhd_2\"]),\n",
    "    net_size_hhd_cum_3 = pl.sum_horizontal([\"net_size_hhd_1\", \"net_size_hhd_2\", \"net_size_hhd_3\"]),\n",
    "    net_size_hhd_cum_4 = pl.sum_horizontal([\"net_size_hhd_1\", \"net_size_hhd_2\", \"net_size_hhd_3\", \"net_size_hhd_4\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Figure 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "fig_filter_d1 = 10\n",
    "fig_filter_d2 = 50\n",
    "fig_filter_d3 = 50\n",
    "fig_filter_d4 = 100\n",
    "\n",
    "data_fig = (\n",
    "    data\n",
    "    # .filter(\n",
    "    #     (pl.col(\"net_size_hhd_1\")<pl.col(\"net_size_hhd_1\").quantile(quantile=0.995, interpolation=\"nearest\")) & \n",
    "    #     (pl.col(\"net_size_hhd_2\")<pl.col(\"net_size_hhd_2\").quantile(quantile=0.995, interpolation=\"nearest\")) & \n",
    "    #     (pl.col(\"net_size_hhd_3\")<pl.col(\"net_size_hhd_3\").quantile(quantile=0.995, interpolation=\"nearest\")) & \n",
    "    #     (pl.col(\"net_size_hhd_4\")<pl.col(\"net_size_hhd_4\").quantile(quantile=0.995, interpolation=\"nearest\"))\n",
    "    # )\n",
    "    .filter(\n",
    "        (pl.col(\"net_size_hhd_1\")<fig_filter_d1) & \n",
    "        (pl.col(\"net_size_hhd_2\")<fig_filter_d2) & \n",
    "        (pl.col(\"net_size_hhd_3\")<fig_filter_d3) & \n",
    "        (pl.col(\"net_size_hhd_4\")<fig_filter_d4)\n",
    "    )\n",
    "    .drop([\"RINPERSOON\",\"hhd_size\",\"net_size_hhd_2\",\"net_size_hhd_3\",\"net_size_hhd_4\"])\n",
    ")\n",
    "\n",
    "cols_fig_y = [\"y_perc_fam_comp\",\"y_perc_contact_comp\",\"y_perc_supp_comp\"]\n",
    "fig_predictors = data_fig.drop(cols_fig_y).select((pl.all()-pl.all().mean()) / pl.all().std()) # Normalized predictors\n",
    "fig_outcomes = data_fig.select(cols_fig_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regressions for figure\n",
    "\n",
    "list_cols_fig_x = [\n",
    "    [\"net_size_hhd_1\",\"overlap_survey_1\"],\n",
    "    [\"net_size_hhd_cum_2\",\"density_2\",\"overlap_survey_2\"],\n",
    "    [\"net_size_hhd_cum_3\",\"density_3\",\"overlap_survey_3\"],\n",
    "    [\"net_size_hhd_cum_4\",\"density_4\",\"overlap_survey_4\"]\n",
    "]\n",
    "\n",
    "dict_results = {}\n",
    "\n",
    "for x in list_cols_fig_x:\n",
    "    # Identify network distance of predictors\n",
    "    dist = x[0][-1]\n",
    "    # Set up regression\n",
    "    reg_x = fig_predictors[x].to_numpy()\n",
    "    reg_x = sm.add_constant(reg_x)\n",
    "    # Create nested dictionary for distance\n",
    "    dict_results[dist] = {}\n",
    "    # Iterate over outcomes to fit the model\n",
    "    for y in cols_fig_y:\n",
    "        # Create nested dictionary for outcome\n",
    "        dict_results[dist][y] = {}\n",
    "        # Set up regression for outcome\n",
    "        reg_y = fig_outcomes[y].to_numpy()\n",
    "        reg = sm.OLS(reg_y, reg_x)\n",
    "        # Fit with robust standard errors\n",
    "        results = reg.fit()\n",
    "        results = results.get_robustcov_results(cov_type=\"HC2\")\n",
    "        # Store results in nested dictionary\n",
    "        dict_results[dist][y][\"predictors\"] = x\n",
    "        dict_results[dist][y][\"coefs\"] = list(results.params)[1:]\n",
    "        dict_results[dist][y][\"se\"] = list(results.bse)[1:]\n",
    "        dict_results[dist][y][\"pvalues\"] = list(results.pvalues)[1:]\n",
    "        dict_results[dist][y][\"constant\"] = {}\n",
    "        dict_results[dist][y][\"constant\"][\"coef\"] = list(results.params)[0]\n",
    "        dict_results[dist][y][\"constant\"][\"se\"] = list(results.bse)[0]\n",
    "        dict_results[dist][y][\"constant\"][\"pvalue\"] = list(results.pvalues)[0]\n",
    "        dict_results[dist][y][\"rsquared\"] = {}\n",
    "        dict_results[dist][y][\"rsquared\"][\"standard\"]= results.rsquared\n",
    "        dict_results[dist][y][\"rsquared\"][\"adjusted\"]= results.rsquared_adj\n",
    "        dict_results[dist][y][\"fstat\"] = {} \n",
    "        dict_results[dist][y][\"fstat\"][\"stat\"] = results.fvalue\n",
    "        dict_results[dist][y][\"fstat\"][\"pvalue\"] = results.f_pvalue\n",
    "        dict_results[dist][y][\"observations\"]= results.nobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "with open(config[\"output\"][\"dict_regressions\"], \"w\") as file:\n",
    "    json.dump(dict_results, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create results dataframes for plot\n",
    "def create_results_df(cols_fig_y, dict_results, dist, type):\n",
    "    df = pl.DataFrame(\n",
    "        {\n",
    "            \"predictors\":dict_results[str(dist)][cols_fig_y[0]][\"predictors\"],\n",
    "            cols_fig_y[0]:dict_results[str(dist)][cols_fig_y[0]][type], \n",
    "            cols_fig_y[1]:dict_results[str(dist)][cols_fig_y[1]][type], \n",
    "            cols_fig_y[2]:dict_results[str(dist)][cols_fig_y[2]][type]\n",
    "        }\n",
    "    )\n",
    "    # Add a density row to the distance 1 results for plot\n",
    "    if f\"density_{dist}\" not in df[\"predictors\"]:\n",
    "        df_density = pl.DataFrame({\"predictors\":[f\"density_{dist}\"],cols_fig_y[0]:[None],cols_fig_y[1]:[None],cols_fig_y[2]:[None]})\n",
    "        df_size = df.head(1)\n",
    "        df_overlap = df.tail(1)\n",
    "        df = pl.concat([df_size,df_density,df_overlap])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_significance(df_pvalues):\n",
    "    '''\n",
    "    Function to recode numerical pvalues to\n",
    "    asterisks representing statistical significance.\n",
    "    '''\n",
    "    for col in df_pvalues.columns:\n",
    "        df_pvalues = (\n",
    "            df_pvalues\n",
    "            .with_columns(pl.when(pl.col(col)<=0.01).then(100).otherwise(pl.col(col)).alias(col))\n",
    "            .with_columns(pl.when((pl.col(col)>0.01) & (pl.col(col)<=0.05)).then(200).otherwise(pl.col(col)).alias(col))\n",
    "            .with_columns(pl.when((pl.col(col)>0.05) & (pl.col(col)<=0.1)).then(300).otherwise(pl.col(col)).alias(col))\n",
    "            .with_columns(pl.when((pl.col(col)>0.1) & (pl.col(col)<100)).then(400).otherwise(pl.col(col)).alias(col))\n",
    "            .with_columns(pl.col(col).cast(pl.Int64).cast(pl.String))\n",
    "            .with_columns(pl.when(pl.col(col)==\"100\").then(pl.lit(\"***\")).otherwise(pl.col(col)).alias(col))\n",
    "            .with_columns(pl.when(pl.col(col)==\"200\").then(pl.lit(\"**\")).otherwise(pl.col(col)).alias(col))\n",
    "            .with_columns(pl.when(pl.col(col)==\"300\").then(pl.lit(\"*\")).otherwise(pl.col(col)).alias(col))\n",
    "            .with_columns(pl.when(pl.col(col)==\"400\").then(pl.lit(\"\")).otherwise(pl.col(col)).alias(col))\n",
    "        )\n",
    "    return df_pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig_6,ax = plt.subplots(2,2,figsize=(10,3.5), layout=\"tight\")\n",
    "\n",
    "xticklabels = [\"% family\", \"% contact\", \"% support\"]\n",
    "yticklabels = [\"Size\",\"Density\",\"Overlap\"]\n",
    "\n",
    "cbar_ax = fig_6.add_axes([1,0.21,0.03,0.45])\n",
    "\n",
    "ax_row = 0\n",
    "ax_col = 0\n",
    "for dist in range(1,5):\n",
    "    # Get coefficients, pvalues, and heatmap annotations\n",
    "    coefs = create_results_df(cols_fig_y, dict_results, dist, \"coefs\")\n",
    "    coefs = coefs.drop(\"predictors\")\n",
    "    annot = coefs.select(pl.all().round(2).cast(pl.String))\n",
    "    pvalues = create_results_df(cols_fig_y, dict_results, dist, \"pvalues\")\n",
    "    pvalues_stars = flag_significance(pvalues.drop(\"predictors\"))\n",
    "    annot = annot + pvalues_stars\n",
    "    # Plot heatmap\n",
    "    sns.heatmap(\n",
    "        coefs,\n",
    "        annot=annot,\n",
    "        fmt=\"\", # necessary for str to be plottable as annot\n",
    "        cbar=dist == 4,\n",
    "        cbar_ax=cbar_ax,\n",
    "        cmap=\"coolwarm\",\n",
    "        vmax=5.08,\n",
    "        vmin=-5.08,\n",
    "        linewidths = 0.8, \n",
    "        linecolor = \"white\",\n",
    "        xticklabels=xticklabels,\n",
    "        yticklabels=yticklabels,\n",
    "        ax=ax[ax_row,ax_col]\n",
    "    )\n",
    "    # Format ticks and titles\n",
    "    ax[ax_row,ax_col].xaxis.tick_top()\n",
    "    ax[ax_row,ax_col].tick_params(axis=\"y\",labelrotation=0)\n",
    "    ax[ax_row,ax_col].set_title(f\"Cumulative distance {dist}\", fontdict={\"fontsize\":10})\n",
    "\n",
    "    # Update ax indexing\n",
    "    if ax_row==0 and ax_col==0:\n",
    "        ax_col+=1\n",
    "    elif ax_row==0 and ax_col==1:\n",
    "        ax_col = ax_col - 1\n",
    "        ax_row = ax_row + 1\n",
    "    elif ax_row==1 and ax_col==0:\n",
    "        ax_col+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save figure\n",
    "fig_6_out = fig_6.get_figure()\n",
    "fig_6_out.savefig(config[\"output\"][\"fig_6_regression\"], bbox_inches = \"tight\", dpi = 400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
